# Summary

Trains and tests a Transformer NN to generate MIDI accompaniments to go along with input MIDI accompaniments. Currently the model and data is set up to generate a drum sequence from melody. The preprocessing of the Lakh data turns the midi files into sequences of a given length tokenized with start, end, and padding tokens for the accompaniment inputs and drum labels. The sequence of tokenized midi melodies is used to train and test the network. FiLM genres can also be added and changed during generation.

This code was created for CSCI 566 Final Project - Team Deep House (Spring 2023)

## Paper

Full paper can be found [here](https://github.com/mike2d2/Transformer-MIDI-Accomp-Gen/blob/main/Deep_Learning_Project_Final_Report.pdf)

## Audio Samples
Here are a couple of audio samples created from drum MIDI

[Audio Sample 1 (No FiLM Applied)](https://drive.google.com/file/d/1x6nMxHL5in6a8Xy_WZJMElSnfEi2HNKN/view?usp=share_link)

[Audio Sample 2 (FiLM Applied)](https://drive.google.com/file/d/1RRqCsbITIEAaQnwx1kecm8XEN15vSjZd/view?usp=share_link)

## Downloading Data

The dataset being used is the [Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/). Specifically, we will use the LMD-matched subset, a collection of 45,000+ tracks spanning many genres that have been matched to entries in the "Million Song Dataset". To get started, download and extract the following data to the `data_raw/` directory:
1. [Zipped MIDI files](http://hog.ee.columbia.edu/craffel/lmd/lmd_matched.tar.gz)
2. [Metadata for song matchings](http://hog.ee.columbia.edu/craffel/lmd/lmd_matched_h5.tar.gz)

```
cd data_raw/
curl -O http://hog.ee.columbia.edu/craffel/lmd/lmd_matched.tar.gz
curl -O http://hog.ee.columbia.edu/craffel/lmd/lmd_matched_h5.tar.gz
tar -xzf lmd_matched.tar.gz
tar -xzf lmd_matched_h5.tar.gz
rm *.tar.gz
```

## Processing Data

Code in the `preprocessing` directory is responsible for organizing, cleaning, and tokenizing raw MIDI files from `data_raw`. The `etl.py` file defines extensions to [MidiTok](https://github.com/Natooz/MidiTok) classes that enable tokenization of MIDI files in the style of [Oore](https://arxiv.org/abs/1808.03715) but with the capability of capturing multiple tracks/instruments.

Generally speaking, the preprocessing steps taken for an arbitrary MIDI file are as follows:
1. Determine if the file has sufficient information for the task (drums + accompaniment)
2. Organize accompaniment instruments into more generic buckets and select the top $N$ by note count (prioritizing piano and bass)
3. Tokenize the drum tracks in the standard style defined by Oore
4. Tokenize each individual accompaniment instrument in the standard style defined by Oore, then concatenate the sequences into a single sequence using "instrument change" events

Files are spliced using `preprocessing/splice.py` before tokenization.

A sample cleaned dataset (the first 1000 valid songs of LMD-matched) is available under the `data_cleaned/` directory. Other cleaned datasets can be created with `preprocessing/process.ipynb`. However, the tokenization takes a long time for large numbers of files, so it is recommended to either use the sample or download a pre-cleaned dataset of the entirety of LMD-matched into `data_cleaned/` from the releases section of the repository.

## Training and Testing

To run training, create and set a directory which will save the weights of the latest epoch. Default is 'saved_models/'. Set the number of epochs by changing NUM_EPOCHS constant in train_film.py. Run:

```
python train_film.py
```
to run the training sequence and save the model. Some configuration may be required to get the script working with your specific model/data.
To test, create and set a directory which will save the midi files generated by the transformer. Default is 'saved_midi/'. Run:
```
python predict_film.py test [genre-list]
```
to select from the testing set and generate midi for their accompaniment. See the source code for additional details.
